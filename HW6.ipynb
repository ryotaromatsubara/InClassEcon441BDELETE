{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N11Ee3GJmywu",
    "outputId": "64e10b54-dad3-48e2-c266-617670a17e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: wikipedia in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from openai) (1.10.13)\n",
      "Requirement already satisfied: sniffio in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ryotaromatsubara/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "Q2A8TGhKm3i5"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E9HEMJSX-3T"
   },
   "source": [
    "# 1.) Set up OpenAI and the enviornment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "4zwwdkZDYDZN"
   },
   "outputs": [],
   "source": [
    "# DONE\n",
    "api_key = \"sk-mfXNLZTqwZVJ66pKcNc9T3VfbkFJsD6EbRJA4xe6rHcddXxQF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "8IiKS0snlpYP"
   },
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key = openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOXc5_BTm9HP"
   },
   "source": [
    "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['API_URL',\n",
       " 'BeautifulSoup',\n",
       " 'Decimal',\n",
       " 'DisambiguationError',\n",
       " 'HTTPTimeoutError',\n",
       " 'ODD_ERROR_MESSAGE',\n",
       " 'PageError',\n",
       " 'RATE_LIMIT',\n",
       " 'RATE_LIMIT_LAST_CALL',\n",
       " 'RATE_LIMIT_MIN_WAIT',\n",
       " 'RedirectError',\n",
       " 'USER_AGENT',\n",
       " 'WikipediaException',\n",
       " 'WikipediaPage',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'cache',\n",
       " 'datetime',\n",
       " 'debug',\n",
       " 'donate',\n",
       " 'exceptions',\n",
       " 'geosearch',\n",
       " 'languages',\n",
       " 'page',\n",
       " 'random',\n",
       " 're',\n",
       " 'requests',\n",
       " 'search',\n",
       " 'set_lang',\n",
       " 'set_rate_limiting',\n",
       " 'set_user_agent',\n",
       " 'stdout_encode',\n",
       " 'suggest',\n",
       " 'summary',\n",
       " 'sys',\n",
       " 'time',\n",
       " 'timedelta',\n",
       " 'unicode_literals',\n",
       " 'util',\n",
       " 'wikipedia']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "-v7OYamHlrEB"
   },
   "outputs": [],
   "source": [
    "page_titles = ['Artificial intelligence','UCLA']\n",
    "page_title = page_titles[0]\n",
    "search_results = wikipedia.search(page_title)\n",
    "page = wikipedia.page(search_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page.content\n",
    "def get_wikipedia_content(page_title):\n",
    "    search_results = wikipedia.search(page_title)\n",
    "    page = wikipedia.page(search_results[0])\n",
    "    return(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "TgY2FkTdmhTH"
   },
   "outputs": [],
   "source": [
    "content = get_wikipedia_content(page_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9aruncMmubX"
   },
   "source": [
    "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The false information found in the article is that UCLA's academic roots were not established in 1881 as a normal school known as the southern branch of the California State Normal School, which later evolved into San José State University. This is inaccurate. UCLA was not a normal school that evolved into San José State University. DONE\n"
     ]
    }
   ],
   "source": [
    "chat_completions = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"I am giving you an article from wikipedia. Please concisely list only the false information found. If there is no false information only return 'DONE'\"},\n",
    "        {\"role\": \"user\", \"content\": content[:8180]}]\n",
    ")\n",
    "print(chat_completions.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "Bmai3B6Dmw3O"
   },
   "outputs": [],
   "source": [
    "def chatgpt_error_correction(text):\n",
    "    chat_completions = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"I am giving you an article from wikipedia. Please concisely list only the false information found. If there is no false information only return 'DONE'\"},\n",
    "    {\"role\": \"user\", \"content\": text[:8180]}]\n",
    "    )\n",
    "    print(chat_completions.choices[0].message.content)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPw5LyPEobmk"
   },
   "source": [
    "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "V7cuhML2ocGn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________Artificial intelligence\n",
      "DONE\n",
      "____________UCLA\n",
      "- The article falsely claims that the academic roots of UCLA were established in 1881 as a normal school known as the southern branch of the California State Normal School, which later evolved into San José State University. This is incorrect, as UCLA was not a branch of San José State University.\n",
      "- The article incorrectly states that UCLA is the most applied-to university in the United States. This is not true.\n",
      "- The article incorrectly claims that UCLA has won 121 NCAA team championships, second only to Stanford University's 128 team titles. The number of championship wins may not be accurate.\n",
      "- The article falsely states that Gabriel Piterberg, a graduate student adviser and professor in the history department, was accused of sexually assaulting two students in 2014. This information is outdated and misleading, as Piterberg was found to have engaged in inappropriate conduct but was not charged with sexual assault.\n"
     ]
    }
   ],
   "source": [
    "page_titles = ['Artificial intelligence','UCLA']\n",
    "for page_title in page_titles:\n",
    "    try:\n",
    "        print(\"____________\" + page_title)\n",
    "        content = get_wikipedia_content(page_title)\n",
    "        chatgpt_error_correction(content)\n",
    "    except:\n",
    "        print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
